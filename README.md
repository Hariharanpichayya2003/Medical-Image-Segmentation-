# Medical-Image-Segmentation-  UNET Architecture
Medical image segmentation aims to provide pixel level semantic interpretation by generating segmentation masks of organs and tumors . Deep learning has been introduced to the field to deal with these problems. Our proposed method is Knowledge distillation, which is a model compression technique that comes under Deep learning.
Recent advances have been made in applying convolutional neural networks to achieve more precise prediction results for medical image segmentation problems. However, the success of existing methods has highly relied on huge computational complexity and massive storage, which is impractical in the real-world scenario.
There is a need to achieve more efficiency when the model is subjected to real – world constraints.
OBJECTIVE OF PROPOSED WORK : 
This architecture empowers the lightweight network to get a significant improvement on segmentation capability while retaining its runtime efficiency. 
The objective is that  the lightweight network should receive at least 32 percent increase in efficiency in the inference phase as compared to previous models.
Knowledge Distillation  is an approach of transferring knowledge from a powerful but cumbersome network to the lightweight model to improve the performance of the latter without affecting its efficiency. Many researchers  utilized it to deal with classification problems by distilling knowledge from the output class probabilities of excellent models. Feature normalized knowledge distillation  gives a good example of optimizing the metric function between the logits exported from the teacher and student networks. 

![image](https://github.com/user-attachments/assets/217e8ba1-1821-42b4-aefc-6d95db90f0df)


